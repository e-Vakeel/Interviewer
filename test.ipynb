{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ANTHROPIC_API_KEY = os.environ.get('ANTHROPIC_API_KEY')\n",
    "print(ANTHROPIC_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import anthropic\n",
    "# import requests\n",
    "# import bs4\n",
    "# from rich import print\n",
    "# import time\n",
    "\n",
    "# client = anthropic.Client(ANTHROPIC_API_KEY)\n",
    "\n",
    "# response = client.beta.prompt_caching.messages.create(\n",
    "#  model=\"claude-3-haiku-2024-03-07\",\n",
    "#  max_tokens=1024,\n",
    "#  system=[\n",
    "#   {\n",
    "#    type=\"text\",\n",
    "#    text=\"Act as our Operating System and DBMS teacher and answer the following questions:\",\n",
    "#   }\n",
    "#  ])\n",
    "##no point in using this as the cache lifetime is only 5 minutes when unused"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "#from vertexai.generative_models import GenerationConfig, GenerativeModel\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "from vertexai.preview import caching\n",
    "\n",
    "from vertexai.generative_models import Part\n",
    "from vertexai.preview import caching\n",
    "import datetime\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "project_id = \"sascha-playground-doit\"\n",
    "vertexai.init(project=project_id, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def measure_time():\n",
    "    start_time = time.perf_counter()\n",
    "    yield\n",
    "    end_time = time.perf_counter()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without using cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "You are an expert book reader, and you answer user's query based on the book you have access to.\n",
    "\"\"\"\n",
    "\n",
    "video = Part.from_uri(\n",
    "    mime_type=\"application/pdf\",\n",
    "    uri=\"./books/OS.pdf\",)\n",
    "\n",
    "model = GenerativeModel(\n",
    "    \"gemini-1.5-pro-001\",\n",
    "    system_instruction=[system_instruction]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with measure_time():\n",
    "  response = model.generate_content(\n",
    "      [video, \"\"\"What is the difference between a process and a thread?\"\"\"],\n",
    "  )\n",
    "  print(response.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "You are an expert video analyzer, and you answer user's query based on the video file you have access to.\n",
    "\"\"\"\n",
    "\n",
    "contents = [\n",
    "    Part.from_uri(\n",
    "    mime_type=\"video/mp4\",\n",
    "    uri=\"gs://doit-ml-demo/gemini/caching/video/Getting started with Gemini on Vertex AI.mp4\")\n",
    "]\n",
    "\n",
    "cached_content = caching.CachedContent.create(\n",
    "    model_name=\"gemini-1.5-pro-001\",\n",
    "    #model_name=\"gemini-1.5-flash-001\",\n",
    "    system_instruction=system_instruction,\n",
    "    contents=contents,\n",
    "    ttl=datetime.timedelta(minutes=60),\n",
    ")\n",
    "\n",
    "cache_name = cached_content.name\n",
    "print(cache_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_content = caching.CachedContent(cached_content_name=cache_name)\n",
    "cached_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = GenerativeModel.from_cached_content(cached_content=cached_content)\n",
    "response = model.generate_content(\"provide a summary for the video\")\n",
    "print(response.usage_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the query best suited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genai_connect():\n",
    "     dotenv.load_dotenv()\n",
    "     genai.configure(api_key=os.environ.get(\"GENAI_API_KEY\"))\n",
    "     model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A process is a separate instance of a running program, while a thread is a unit of execution within a process. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "**Process:**\n",
      "\n",
      "* A process has its own memory space, resources, and address space.\n",
      "* Processes are independent entities that cannot directly access each other's data.\n",
      "* Communication between processes requires explicit mechanisms like inter-process communication (IPC).\n",
      "* Launching a process is a heavyweight operation that consumes more system resources than creating a thread.\n",
      "* Example: Opening a new application on your computer creates a separate process.\n",
      "\n",
      "**Thread:**\n",
      "\n",
      "* A thread shares the same memory space and resources with other threads within the same process.\n",
      "* Threads can communicate and share data directly within the process.\n",
      "* Creating and managing threads is a lightweight operation compared to processes.\n",
      "* Multiple threads within a process can run concurrently, providing a form of parallelism.\n",
      "* Example: A web server might use multiple threads to handle multiple client requests simultaneously.\n",
      "\n",
      "**In essence:**\n",
      "\n",
      "Think of a process as a car, and threads as the passengers inside. All passengers in the car share the same journey, but they can move around and perform actions independently within the car. \n",
      "\n",
      "Processes are isolated entities, while threads are collaborative units within a larger context.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai_connect()\n",
    "response = model.generate_content(\"What is the difference between a process and a thread? Reply with text and not markdown.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using Anthropic API for CN and OOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "# Create an instance of the Anthropics API client\n",
    "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)  \n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_claude(query):\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1024,\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        system=\"You are an expert in computer networks and Object Oriented Programming, and you answer user's multiple choice questions based on your knowledge. Keep your answers concise mentioning only the option that you think is the correct answer , don't mention the reason and answer with at most 10 words.\",\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ask_claude(\" A proxy server is used as the computer? the options are  external access acting as a backup performing file handling accessing user permissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
